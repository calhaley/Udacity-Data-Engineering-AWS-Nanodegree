# Udacity-Data-Engineering-AWS-Nanodegree
* Learn to design data models, build data warehouses and data lakes, automate data pipelines, and work with massive datasets.

* Create user-friendly relational and NoSQL data models
* Create scalable and efficient data warehouses
* Work efficiently with massive datasets
* Build and interact with a cloud-based data lake
* Automate and monitor data pipelines
* Develop proficiency in Spark, Airflow, and AWS tools

# Projects
---
<ins>Cassandra ETL</ins>
* In this project, we apply Data Modeling with Cassandra and build an ETL pipeline using Python. We will build a Data Model around our queries that we want to get answers for.

<ins> Data Warehousing with AWS Redshift</ins>
* This project creates a data warehouse, in AWS Redshift. A data warehouse provides a reliable and consistent foundation for users to query and answer some business questions based on requirements.

<ins> Data Lake with Spark & AWS S3 </ins>
* Use Spark and AWS Glue allow you to process data from multiple sources, categorize the data, and curate it to be queried in the future for multiple purposes.
* Build a data lakehouse solution for sensor data that trains a machine learning model.

<ins> Data Pipelining with Airflow </ins>
* This project schedules data pipelines, to perform ETL from json files in S3 to Redshift using Airflow. 
* Why use Airflow? Airflow allows workflows to be defined as code, they become more maintainable, versionable, testable, and collaborative
